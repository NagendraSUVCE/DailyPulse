{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capstone 2 Insurance Claim Approval Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Load Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%run ./.setup/learner_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Core Python libraries\n",
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "import textwrap\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "import httpx\n",
    "from dotenv import load_dotenv\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# HTTP and retry logic\n",
    "import httpx\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "# Display utilities for Jupyter\n",
    "from IPython.display import display, Image, Markdown\n",
    "\n",
    "# LangChain & LangGraph core\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.tools import tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    trim_messages,\n",
    ")\n",
    "from langchain_core.messages.utils import count_tokens_approximately\n",
    "\n",
    "# LangGraph components\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import (\n",
    "    create_react_agent,\n",
    "    ToolNode,\n",
    "    tools_condition,\n",
    ")\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import Command\n",
    "# Typing\n",
    "from typing import Annotated, Literal\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. Create model Client\n",
    "Below is the code to set-up the UAIS environment for establishing the connection to LLM and to get authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Authentication:\n",
    "def get_access_token():\n",
    "    auth = \"https://api.uhg.com/oauth2/token\"\n",
    "    scope = \"https://api.uhg.com/.default\"\n",
    "    grant_type = \"client_credentials\"\n",
    "\n",
    "    with httpx.Client() as client:\n",
    "        body = {\n",
    "            \"grant_type\": grant_type,\n",
    "            \"scope\": scope,\n",
    "            \"client_id\": dbutils.secrets.get(scope=\"AIML_Training\", key=\"client_id\"),\n",
    "            \"client_secret\": dbutils.secrets.get(scope=\"AIML_Training\", key=\"client_secret\"),\n",
    "        }\n",
    "        headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "        resp = client.post(auth, headers=headers, data=body, timeout=60)\n",
    "        access_token = resp.json()[\"access_token\"]\n",
    "        return access_token\n",
    "    \n",
    "\n",
    "load_dotenv('./Data/UAIS_vars.env')\n",
    "\n",
    "endpoint = os.environ.get(\"MODEL_ENDPOINT\")\n",
    "model_name = os.environ.get(\"MODEL_NAME\")\n",
    "project_id = os.environ.get(\"PROJECT_ID\")\n",
    "api_version = os.environ.get(\"API_VERSION\")\n",
    "\n",
    "\n",
    "chat_client = openai.AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_version=api_version,\n",
    "        azure_deployment=model_name,\n",
    "        azure_ad_token=get_access_token(),\n",
    "        default_headers={\n",
    "            \"projectId\": project_id\n",
    "        }\n",
    "    )\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "OPENAI_API_VERSION = os.environ[\"OPENAI_API_VERSION\"]\n",
    "EMBEDDINGS_DEPLOYMENT_NAME = os.environ[\"EMBEDDINGS_DEPLOYMENT_NAME\"]\n",
    "MODEL_DEPLOYMENT_NAME = os.environ[\"MODEL_DEPLOYMENT_NAME\"]\n",
    "PROJECT_ID = os.environ['PROJECT_ID']\n",
    "\n",
    "chat_client = AzureChatOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    azure_deployment=MODEL_DEPLOYMENT_NAME,\n",
    "    temperature=0,\n",
    "    azure_ad_token=get_access_token(),\n",
    "    default_headers={\n",
    "        \"projectId\": PROJECT_ID\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "embeddings_client = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    azure_deployment=EMBEDDINGS_DEPLOYMENT_NAME,\n",
    "    azure_ad_token=get_access_token(),\n",
    "    default_headers={\n",
    "        \"projectId\": PROJECT_ID\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. Load and Inspect Validation Patient Data\n",
    "We begin by loading the validation patient records directly from the JSON file as python dictonaries. It is easy to inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load from files\n",
    "with open('Data/validation_records.json') as f:\n",
    "    sample_patients = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# sample records\n",
    "sample_patients[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Implement utility function for computing age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def compute_age(dob: str, reference_date: str) -> int:\n",
    "    dob_dt = datetime.strptime(dob, \"%Y-%m-%d\")\n",
    "    ref_dt = datetime.strptime(reference_date, \"%Y-%m-%d\")\n",
    "    age = ref_dt.year - dob_dt.year - ((ref_dt.month, ref_dt.day) < (dob_dt.month, dob_dt.day))\n",
    "    return age\n",
    "\n",
    "# Example usage:\n",
    "age = compute_age(\"1980-05-15\", \"2025-09-21\")\n",
    "display(age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for patient in sample_patients:\n",
    "    dob = patient.get(\"date_of_birth\")\n",
    "    dos = patient.get(\"date_of_service\")\n",
    "    if dob and dos:\n",
    "        patient[\"age\"] = compute_age(dob, dos)\n",
    "    else:\n",
    "        patient[\"age\"] = None\n",
    "\n",
    "sample_patients[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Load and inspect insurance policy data\n",
    "Data from insurance_policies.json is loaded here. This preserves all metadata and makes it easy to inspect policy information and coverage details for further processing by agent later on as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load from files\n",
    "with open('Data/insurance_policies.json') as f:\n",
    "    insurance_policies = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# looking at sample insurance policy records\n",
    "insurance_policies[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4. Reference codes\n",
    "We load the reference codes from reference_codes.json JSON file as python dictionary. This dictionary contains CPT(procedure) and ICD-10 (diagnosis) codes with human readable descriptions. This will be fed into our agent to improve the quality of reports and summaries created which will aid in its reasoning process. Data from Reference code is loaded here. CPT_CODES and ICD10_CODES are inspected here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load from files\n",
    "with open('Data/reference_codes.json') as f:\n",
    "    ref = json.load(f)\n",
    "    CPT_CODES = ref['CPT']\n",
    "    ICD10_CODES = ref['ICD10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Look at CPT Codes\n",
    "CPT_CODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#look at ICD 10 code\n",
    "ICD10_CODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Create Dictionary for easy data access and look ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create DB Look ups\n",
    "PATIENT_DB = {p[\"patient_id\"]: p for p in sample_patients}\n",
    "POLICY_DB = {p[\"policy_id\"]: p for p in insurance_policies}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create and Define Tools for the ReAct Agent\n",
    "In this section we define the three tools powering our ReAct Agent:\n",
    "\n",
    "summarize_patient_record: Summarizes a patient record in a structured format using LLM reasoning. Returns key points for use in downstream eligibility reasoning.\n",
    "\n",
    "summarize_policy_guideline: Summarizes an insurance policy in a structured format using LLM reasoning. Returns clearly defined conditions for claim coverage.\n",
    "\n",
    "check_claim_coverage: Uses LLM reasoning to assess whether each claimed procedure in the patient's record satisfies the policy's coverage conditions.\n",
    "\n",
    "This tool does not make final denial decisions\n",
    "\n",
    "If all requirements are satisfied, the procedure is approved\n",
    "If any requirement is not met, the case is routed for further human review. Only procedures and diagnoses explicitly claimed by the patient are evaluated.\n",
    "Returns a structured, step-by-step coverage analysis along with a recommendation per procedure: APPROVE or ROUTE FURTHER REVIEW.\n",
    "3.1. Define Tool for Summarizing Patient Records\n",
    "This tool uses LLM-powered reasoning to generate summaries of patient records along with their insurance claim data. The summaries follow a clear, structured format to support consistent and accurate downstream eligibility reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.1 Create a function to build prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the prompt and check for sample patient. Verify the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def build_prompt_summarize_patient_record(record, icd10_codes, cpt_codes):\n",
    "    return f\"\"\"\n",
    "You are a medical claims summarization assistant. Given a patient insurance claim record, generate a structured summary with the following SEVEN labeled sections, in this order:\n",
    "\n",
    "1. Patient Demographics: Include name, gender, and age (use \"age\" field if present, otherwise compute from date_of_birth and date_of_service).\n",
    "2. Insurance Policy ID\n",
    "3. Diagnoses and Descriptions: List all ICD-10 codes and their mapped descriptions.\n",
    "4. Procedures and Descriptions: List all CPT codes and their mapped descriptions.\n",
    "5. Preauthorization Status: Clearly state if preauthorization was required and whether it was obtained.\n",
    "6. Billed Amount (in USD)\n",
    "7. Date of Service\n",
    "\n",
    "Use the following ICD-10 code mappings: {json.dumps(icd10_codes)}\n",
    "Use the following CPT code mappings: {json.dumps(cpt_codes)}\n",
    "\n",
    "Here is the patient record:\n",
    "{json.dumps(record)}\n",
    "\n",
    "Return only the structured summary, clearly formatted with each section labeled.\n",
    "\"\"\"\n",
    "prompt = build_prompt_summarize_patient_record(PATIENT_DB['P011'], ICD10_CODES, CPT_CODES)\n",
    "print(prompt)\n",
    "prompt_messages = [\n",
    "    {\"role\": \"developer\", \"content\": prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"Patient review text:\\n\\n{PATIENT_DB['P011']}\"}\n",
    "]\n",
    "response = chat_client.invoke(prompt_messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.2 Create function summarize_patient_record\n",
    "This tool is responsible for extracting a structured summary of a patient’s insurance claim record using LLM reasoning. It accepts a raw patient record (as a JSON string or plain string) and returns a well-structured summary report that will later be used for claim coverage evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Tool 1: Summarizing Patient  Health  Record with  Insurance  Claim\n",
    "@tool\n",
    "def summarize_patient_record(record_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Summarizes a patient's health record and insurance claim information from a JSON string input.\n",
    "    \"\"\"\n",
    "        # Parse input string to dict\n",
    "    try:\n",
    "        prompt = build_prompt_summarize_patient_record(record_str, ICD10_CODES, CPT_CODES)\n",
    "        # print(prompt)\n",
    "        prompt_messages = [\n",
    "            {\"role\": \"developer\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Patient review text:\\n\\n{PATIENT_DB['P011']}\"}\n",
    "        ]\n",
    "        response = chat_client.invoke(prompt_messages)\n",
    "        return (response.content)\n",
    "    except Exception:\n",
    "        raise ValueError(\"Input must be a valid JSON string representing a patient record.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test tool to check sample patient record summary\n",
    "This section uses the summarize_patient_record tool on a specific patient record (in this case , the patient with ID 'P011') and displays the generated summary report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "patient_record_json = json.dumps(PATIENT_DB['P011'])\n",
    "patient_summary = summarize_patient_record.invoke(patient_record_json)\n",
    "display(Markdown(patient_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. Define Tool for Summarizing Insurance Policy Guideline\n",
    "This tool uses LLM reasoning to summarize the claim coverage rules and conditions of an insurance policy, ensuring that the claim coverage process follows the defined criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def summarize_policy_guideline(policy_id: str) -> str:\n",
    "    \"\"\"Summarizes the policy details.\"\"\"\n",
    "    prompt = (\n",
    "        f\"You are an insurance policy assistant. Given a policy id {policy_id}, extract a summary from it. \"\n",
    "        f\"You need to refer {insurance_policies} to gather information about that policy id. \"\n",
    "        \"You have to include the following sections in the order provided:\\n\"\n",
    "        \"• Policy Details: policy ID and plan name\\n\"\n",
    "        \"• Covered Procedures:\\n\"\n",
    "        \"    For each covered procedure listed in the policy, include the following sub-points:\\n\"\n",
    "        f\"    o Procedure Code and Description (Use {CPT_CODES} for reference)\\n\"\n",
    "        f\"    o Covered Diagnoses and Descriptions (Use {ICD10_CODES} for reference)\\n\"\n",
    "        \"    o Gender Restriction\\n\"\n",
    "        \"    o Age Range\\n\"\n",
    "        \"    o Preauthorization Requirement\\n\"\n",
    "        \"    o Notes on Coverage (if any)\\n\"\n",
    "    )\n",
    "    response = chat_client.invoke(prompt)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test tool to check sample insurance policy guideline summary\n",
    "This section runs the summary_policy_guideline function to summarize the policy details for the given policy ID (POL1007) and displays the summary report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "POLICY_DB['POL1002']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "policy_summary = summarize_policy_guideline.invoke(\"POL1002\")\n",
    "display(Markdown(policy_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3. Define Tool for Claim Coverage Check\n",
    "This tool uses LLM reasoning to assess whether each claimed procedure in the patient's record satisfies the coverage conditions outlined in their insurance policy. It relies on structured summaries of both the patient record and the policy guideline.\n",
    "\n",
    "Only procedures and diagnoses explicitly claimed by the patient are evaluated.\n",
    "\n",
    "The tool does not issue final denials.\n",
    "\n",
    "If all policy requirements are met, the procedure is marked as APPROVE.\n",
    "If any requirement is not met, the procedure is flagged as ROUTE FOR REVIEW for expert human decision-making.\n",
    "The tool returns a clear, step-by-step coverage analysis and recommendation for each claimed procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@tool\n",
    "def check_claim_coverage(patient_record_summary: str, policy_summary: str) -> str:\n",
    "    \"\"\"\n",
    "    Evaluates whether the patient's claimed procedure is covered under their insurance policy.\n",
    "    Returns a structured decision with reasoning.\n",
    "\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a claims coverage validation assistant.\n",
    "Your task is to determine whether the patient's claimed procedure is covered under their insurance policy. Given a patient claim summary {patient_record_summary} and the summary of the insurance policy the patient has opted for {policy_summary}, determine if the claim is covered under the policy. You have to return a coverage eligibility decision, either approval or routing for review.: \n",
    "A procedure should be approved only if all the below conditions are met:\n",
    "        • The patient's diagnosis code(s) match the policy-covered diagnoses for the claimed procedure.\n",
    "        • The procedure code is explicitly listed in the policy, and all associated conditions are satisfied.\n",
    "        • The patient's age falls within the policy's defined age range (inclusive of the lower bound, exclusive of the upper bound).\n",
    "        • The patient's gender matches the policy's requirement for that procedure.\n",
    "        • If preauthorization is required by the policy, it must have been obtained.\n",
    "        \n",
    "        Do **not** infer or assume any missing information.\n",
    "        Do **not** evaluate procedures or diagnoses not explicitly listed in the patient record.\n",
    "         If any required condition is missing or ambiguous, route the case for **manual review**.\n",
    "\n",
    "        The response should contain the following points:\n",
    "        • Coverage Review: Step-by-step analysis for the claimed procedure, detailing the checks performed. (each patient has only one procedure)\n",
    "        • Summary of Findings: Summary of which coverage requirements were met or not met.\n",
    "        • Final Decision: For each procedure for the claim, return either \"APPROVE\" or \"ROUTE FOR REVIEW\" with a brief explanation of the reason behind it.\n",
    "        \"\"\"\n",
    "        \n",
    "    response = chat_client.invoke(prompt)\n",
    "    return {\"Coverage eligibility decision\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result = check_claim_coverage.invoke({\n",
    "    'patient_record_summary': patient_summary,\n",
    "    'policy_summary': policy_summary\n",
    "})\n",
    "# Extract the string from the dictionary\n",
    "markdown_text = result['Coverage eligibility decision']\n",
    "\n",
    "display(Markdown(markdown_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools for Claims Processing Agent\n",
    "In this section, we define the tools that will be used by the agent to:\n",
    "\n",
    "Process patient records\n",
    "Summarize policy guidelines\n",
    "Check claim coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Set Up the System Instruction Prompt for the Claims Approval Agent\n",
    "This system prompt defines the agent's end-to-end workflow and enforces strict compliance with claims coverage determination procedures.\n",
    "\n",
    "It explicitly specifies:\n",
    "\n",
    "The tools the agent must use\n",
    "The exact sequence of steps to follow\n",
    "The required output format for determining claim approval or denial\n",
    "This ensures:\n",
    "\n",
    "Consistent, audit-ready claim processing\n",
    "Elimination of ambiguity in agent decision-making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Instruction prompt for the overall Agent\n",
    "\n",
    "AGENT_PROMPT_TXT = \"\"\"\n",
    "You are an Insurance Claims Reviewer Assistant. Your task is to determine whether a patient's insurance claim should be APPROVED or ROUTED FOR REVIEW, strictly following the policy guidelines.\n",
    "\n",
    "You have access to ONLY the following three tools, which must be used in this exact order:\n",
    "1. summarize_patient_record(record)\n",
    "2. summarize_policy_guideline(policy_id)\n",
    "3. check_claim_coverage(patient_record_summary, policy_summary)\n",
    "\n",
    "Your workflow:\n",
    "- Step 1: Use summarize_patient_record to generate a structured summary of the patient claim record.\n",
    "- Step 2: Extract the policy_id from the patient summary and use summarize_policy_guideline to summarize the insurance policy.\n",
    "- Step 3: Use check_claim_coverage with the patient record summary and policy summary to determine coverage eligibility.\n",
    "\n",
    "Once both the summaries are obtained, check if the claim should be approved or not under the policy guidelines based on the claims summary and the policy summary.\n",
    "Call the tools in this way mentioned above and provide the most appropriate response. First call the summarize_patient_record tool, then summarize_policy_guideline, and finally check_claim_coverage.\n",
    "\n",
    "Do NOT call any tool more than once per claim. Do NOT repeat or loop tool calls. Do NOT attempt to answer without using all three tools in the specified order.\n",
    "\n",
    "if you get \"Sorry, need more steps to process this request.\" as the ouput , mark the decision as ROUTE FOR REVIEW, and provide suitable reason and mention somewhere it is \"Tool limitation\"\n",
    "\n",
    "Your final response must be formatted as follows:\n",
    " - Decision: decision should be APPROVE or ROUTE FOR REVIEW. (Decision heading should be in bold)\n",
    " - Reason: A concise explanation (max 4 sentences) referencing specific coverage rules, policy conditions, diagnosis and procedure codes/descriptions, and any relevant age or gender criteria that led to your decision.\n",
    "\n",
    "Strictly follow this workflow and output format for every claim.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create LangGraph Claims Approval ReAct Agent\n",
    "This step creates the ReAct agent and equips it with the necessary tools, LLM, and system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# List of all tools that the LLM should be aware of\n",
    "tools = [summarize_patient_record, summarize_policy_guideline, check_claim_coverage]\n",
    "\n",
    "AGENT_SYS_PROMPT = SystemMessage(content=AGENT_PROMPT_TXT)\n",
    "# Create the agent using tools, LLM, and the system instruction prompt\n",
    "agent = create_react_agent(\n",
    "    model=chat_client,\n",
    "    tools=tools,\n",
    "    prompt=AGENT_SYS_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Define Utility Function to Call the Agent\n",
    "This utility function is used to interact with the agent, stream its step-by-step reasoning, and display the final response in Markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def call_agent(agent, query, verbose=False, config=None):\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    # Stream the agent's execution for the given query\n",
    "    for event in agent.stream(\n",
    "        {\"messages\": [HumanMessage(content=query)]},  # Input prompt\n",
    "        stream_mode='values',  # Stream output as intermediate values\n",
    "        config=config\n",
    "    ):\n",
    "        # If verbose is enabled, print each intermediate message\n",
    "        if verbose:\n",
    "            event[\"messages\"][-1].pretty_print()\n",
    "    # Display the final response from the agent as Markdown\n",
    "    print('\\n\\nFinal Response:\\n')\n",
    "    display(Markdown(event[\"messages\"][-1].content))\n",
    "    # Return the final message content for optional downstream use\n",
    "    return event[\"messages\"][-1].content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Run the Agent on a Validation Patient Data\n",
    "In this section, we run the agent on all the sample patient records from validation_records.json. The goal is to observe how the agent processes these claims and ensure that everything is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "PATIENT_DB['P011']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# run in for a single patient\n",
    "patient_record = str(PATIENT_DB['P011'])  # converting dict to string\n",
    "response = call_agent(agent=agent,\n",
    "                      query=f\"Evaluate this claim: {patient_record}\",\n",
    "                      verbose=True,\n",
    "                      config = {\"recursion_limit\": 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "PATIENT_DB.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Validate Agent Performance Using Validation Human Reference Data\n",
    "We save the agent responses in a dataframe\n",
    "Load the validation_reference_results.csv\n",
    "Check agent performance by comparing results manually or using a simple LLM-as-judge prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "validation_agent_results = pd.DataFrame({\n",
    "    'patient_id': list(PATIENT_DB.keys()),\n",
    "    'generated_response': agent_responses\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "validation_human_results = pd.read_csv(\"Data/validation_reference_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# You can merge the dataframes and validate manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "validation_merged_df = validation_agent_results.merge(validation_human_results, on='patient_id', how='inner')\n",
    "validation_merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR use LLM as a judge with creating your own prompt and validate the agendt performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Grading prompt template\n",
    "grading_prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert insurance claims adjudicator.\n",
    "Your task is to evaluate whether the AI-generated response matches the human reference response for the same claim.\n",
    "Human Response:\n",
    "    {reference}\n",
    "    Model-Generated Response:\n",
    "    {generated}\n",
    "\n",
    "**Grading Rules:**\n",
    "\n",
    "> - Grade as \"Correct\" if the generated response matches the reference response in **semantic meaning, factual accuracy, decision outcome (e.g., approve/deny), and reasoning based on policy terms and claim details.** Minor differences in wording are acceptable as long as the core rationale and adjudication logic are aligned.\n",
    "\n",
    "- The wording of the reason may differ, but as long as the core rationale remains consistent (e.g., policy coverage mismatch, age/gender requirements, diagnosis not aligning), the response can still be graded as \"Correct.\"\n",
    "\n",
    "> - Grade as \"Incorrect\" if the generated response: **differs in decision outcome, contains factual inconsistencies, misinterprets policy terms, omits key reasoning, or introduces irrelevant or misleading information.**\n",
    "\n",
    "- Focus on **content similarity**, **factual alignment**, and **coherence of reasoning** when comparing the responses.\n",
    "\n",
    "Respond in JSON format:\n",
    "{{\n",
    "    \"grade\": \"Correct\" or \"Incorrect\",\n",
    "    \"justification\": \"Brief justification here.\"\n",
    "}}\n",
    "\n",
    "Reference response:\n",
    "{reference}\n",
    "\n",
    "Generated response:\n",
    "{generated}\n",
    "\"\"\")\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "\n",
    "for idx, row in validation_merged_df.iterrows():\n",
    "    prompt = grading_prompt_template.format(\n",
    "        reference=row[\"reference_response\"],\n",
    "        generated=row[\"generated_response\"]\n",
    "    )\n",
    "\n",
    "    # Send to LLM and get response\n",
    "    llm_response = chat_client.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    # If the response is in markdown/code block, strip those\n",
    "    import re\n",
    "    response_content = llm_response.content.strip()\n",
    "    response_content = re.sub(r\"^```json|```$\", \"\", response_content).strip()\n",
    "    \n",
    "    try:\n",
    "        eval_result = eval(response_content)  # Or use json.loads if safe\n",
    "    except Exception:\n",
    "        eval_result = {\"grade\": \"Error\", \"justification\": response_content}\n",
    "    \n",
    "    results.append({\n",
    "        \"patient_id\": row[\"patient_id\"],\n",
    "        \"grade\": eval_result.get(\"grade\", \"\"),\n",
    "        \"justification\": eval_result.get(\"justification\", \"\")\n",
    "    })\n",
    "\n",
    "# run prompt against agent response and human reference response and pass to LLM\n",
    "\n",
    "# Create a DataFrame with the grading results\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on a above it is clear that the agent is working as expected and now we can generate responses on test patient records for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "9. Generate Responses on Test Data\n",
    "We load the test_records.json\n",
    "Generate results from agent\n",
    "Store in submission.csv for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with open('Data/test_records.json') as f:\n",
    "    test_patients = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "len(test_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_patients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for patient in test_patients:\n",
    "    dob = patient.get(\"date_of_birth\")\n",
    "    dos = patient.get(\"date_of_service\")\n",
    "    if dob and dos:\n",
    "        patient[\"age\"] = compute_age(dob, dos)\n",
    "    else:\n",
    "        patient[\"age\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_patients[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_patient_ids = [patient['patient_id'] for patient in test_patients]\n",
    "test_patient_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_responses = []\n",
    "\n",
    "for patient in test_patients:\n",
    "    print(f'Processing for patient {patient[\"patient_id\"]}...')\n",
    "    patient_record = str(patient)\n",
    "    response = call_agent(agent,\n",
    "                          f\"Evaluate this claim: {patient_record}\",\n",
    "                          verbose=True,\n",
    "                          config = {\"recursion_limit\": 50})\n",
    "    test_responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv',  # use the correct file name - submission.csv\n",
    "                     index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('submission.csv')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
